{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1270e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all file count= 2098\n",
      "train data: (33568, 28, 28) (33568,)\n",
      "tran data shape: (33568, 28, 28) (33568, 10)\n",
      "all file count= 207\n",
      "test data: (207, 28, 28) (207,)\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import matplotlib.image as pltimage\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import torch, torchvision\n",
    "\n",
    "def show_files(label_dir):\n",
    "    all_files = []\n",
    "    file_list = os.listdir(label_dir)\n",
    "    for file in file_list:\n",
    "        cur_path = os.path.join(label_dir, file)\n",
    "        root, extension = os.path.splitext(cur_path)\n",
    "        ext_lower = extension.lower()\n",
    "        if ext_lower == \".png\" or ext_lower == \".jpg\" or ext_lower == \".jpeg\":\n",
    "            all_files.append(cur_path)\n",
    "    return all_files\n",
    "\n",
    "def obtainXY(data_dir, augmented=False, shuffle=False):\n",
    "    X = []\n",
    "    Y = []\n",
    "    classes =  [i for i in range(10)]\n",
    "    all_file_count = 0\n",
    "    for label in classes:\n",
    "        all_files = show_files(data_dir + str(label))\n",
    "        all_file_count += len(all_files)\n",
    "        for file in all_files:\n",
    "            org = Image.open(file)\n",
    "            # 需要数据增广\n",
    "            w, h, c = (*org.size, len(org.mode))\n",
    "            # 数据增广配置\n",
    "            trans = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.RandomAffine(translate=(0.25, 0.25), scale=(0.8, 1.2), degrees=40, interpolation=torchvision.transforms.functional.InterpolationMode.NEAREST),\n",
    "                torchvision.transforms.RandomHorizontalFlip(p=0.2),\n",
    "                torchvision.transforms.RandomCrop(size=(h, w))])\n",
    "            # 增广15倍，带原图16张\n",
    "            for i in range(16):\n",
    "                if i == 0:\n",
    "                    img = org\n",
    "                else:\n",
    "                    img = trans(org)\n",
    "                img = img.resize((28, 28), Image.ANTIALIAS)\n",
    "                gray = img.convert(\"L\")\n",
    "                x = np.asarray(gray)/255.\n",
    "                y = label\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "                if not augmented: # 不需要增广时，只用原图\n",
    "                    break\n",
    "    print(\"all file count=\", all_file_count)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    if shuffle:\n",
    "        indexs = [i for i in range(len(X))] \n",
    "        random.shuffle(indexs) #乱序\n",
    "        X = X[indexs]\n",
    "        Y = Y[indexs]\n",
    "    return X, Y\n",
    "\n",
    "dataset_dir = \"./dataset/\"\n",
    "X, Y = obtainXY(dataset_dir, augmented=True, shuffle=True)\n",
    "print(\"train data:\", X.shape, Y.shape)\n",
    "\n",
    "train_x = X\n",
    "# 将label转成向量，如 2 => [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "train_y = tf.keras.utils.to_categorical(Y, num_classes=10)\n",
    "# input output shape\n",
    "print(\"tran data shape:\", train_x.shape, train_y.shape)\n",
    "\n",
    "dataset_test_dir = \"./dataset-test/\"\n",
    "vX, vY = obtainXY(dataset_test_dir)\n",
    "print(\"test data:\", vX.shape, vY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "494583cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({7: 3648, 6: 3472, 9: 3472, 5: 3456, 4: 3424, 3: 3376, 0: 3312, 1: 3232, 8: 3216, 2: 2960}) 33568\n",
      "0 占比: 0.099\n",
      "1 占比: 0.096\n",
      "2 占比: 0.088\n",
      "3 占比: 0.101\n",
      "4 占比: 0.102\n",
      "5 占比: 0.103\n",
      "6 占比: 0.103\n",
      "7 占比: 0.109\n",
      "8 占比: 0.096\n",
      "9 占比: 0.103\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_pecent(data, class_num=10):\n",
    "    y_ = list(data)\n",
    "    size = len(y_)\n",
    "    class_counter = Counter(y_)\n",
    "    print(class_counter, size)\n",
    "    for i in range(class_num):\n",
    "        print(i, \"占比:\", round(class_counter[i]/size, 3))\n",
    "\n",
    "print_pecent(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89f4e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "# 全连接网络\n",
    "def creat_nn(print_summary=False):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "# 卷积神经网络\n",
    "def creat_cnn(print_summary=False):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), strides=(2, 2), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.Conv2D(16, (3, 3), strides=(2, 2), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "# 训练模型\n",
    "def train_net(train_x, train_y, batch_size, epochs, use_cnn=False, load_save_model=False):\n",
    "    if use_cnn:\n",
    "        tflite_name = \"mymodel-cnn.tflite\"\n",
    "        model_path = './model-cnn/'\n",
    "    else:\n",
    "        tflite_name = \"mymodel.tflite\"\n",
    "        model_path = './model/'\n",
    "    if load_save_model:\n",
    "        print('train load save model path', model_path)\n",
    "        model = load_model(model_path)\n",
    "    else:\n",
    "        print('train create new model')\n",
    "        if use_cnn:\n",
    "            model = creat_cnn(print_summary=True) # cnn卷积神经网络\n",
    "        else:\n",
    "            model = creat_nn(print_summary=True) # 全连接网络\n",
    "    model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "    model.save(model_path)\n",
    "    \n",
    "    #save tflite model\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    open(tflite_name, \"wb\").write(tflite_model)\n",
    "    return model\n",
    "\n",
    "# 验证模型\n",
    "def verify_model(model, valid_x, valid_y, num_classes=10):\n",
    "    y_pred = model.predict(valid_x)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    size = [0 for i in range(num_classes)]\n",
    "    correct = [0 for i in range(num_classes)]\n",
    "    for i in range(len(y_pred)):\n",
    "        label = valid_y[i]\n",
    "        pred = y_pred[i]\n",
    "        size[label] += 1\n",
    "        if pred == label:\n",
    "            correct[label] += 1\n",
    "    print(\"total accuracy:\", sum(correct)/sum(size))\n",
    "    for i in range(num_classes):\n",
    "        print(\"label:\", str(i), \" accuracy:\", correct[i]/max(size[i], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6458b31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train create new model\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               100480    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 1.8426 - accuracy: 0.3574\n",
      "Epoch 2/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 1.2811 - accuracy: 0.5768\n",
      "Epoch 3/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 1.0618 - accuracy: 0.6509\n",
      "Epoch 4/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.9388 - accuracy: 0.6949\n",
      "Epoch 5/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.8651 - accuracy: 0.7174\n",
      "Epoch 6/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.8067 - accuracy: 0.7371\n",
      "Epoch 7/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.7641 - accuracy: 0.7510\n",
      "Epoch 8/24\n",
      "2098/2098 [==============================] - 2s 1ms/step - loss: 0.7267 - accuracy: 0.7635\n",
      "Epoch 9/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.6888 - accuracy: 0.7742\n",
      "Epoch 10/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.6635 - accuracy: 0.7819\n",
      "Epoch 11/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.6441 - accuracy: 0.7894\n",
      "Epoch 12/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.6201 - accuracy: 0.7961\n",
      "Epoch 13/24\n",
      "2098/2098 [==============================] - 2s 1ms/step - loss: 0.6015 - accuracy: 0.8008\n",
      "Epoch 14/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.5839 - accuracy: 0.8057\n",
      "Epoch 15/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.5688 - accuracy: 0.8129\n",
      "Epoch 16/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.5583 - accuracy: 0.8140\n",
      "Epoch 17/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.5462 - accuracy: 0.8197\n",
      "Epoch 18/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.5252 - accuracy: 0.8243\n",
      "Epoch 19/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.5204 - accuracy: 0.8242\n",
      "Epoch 20/24\n",
      "2098/2098 [==============================] - 2s 1ms/step - loss: 0.5049 - accuracy: 0.8321\n",
      "Epoch 21/24\n",
      "2098/2098 [==============================] - 2s 1ms/step - loss: 0.5008 - accuracy: 0.8330\n",
      "Epoch 22/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.4907 - accuracy: 0.8365\n",
      "Epoch 23/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.4802 - accuracy: 0.8390\n",
      "Epoch 24/24\n",
      "2098/2098 [==============================] - 3s 1ms/step - loss: 0.4654 - accuracy: 0.8431\n",
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/yn/ntp3qk1946g09h3hckj58phm0000gp/T/tmp8ekk6u17/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/yn/ntp3qk1946g09h3hckj58phm0000gp/T/tmp8ekk6u17/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step\n",
      "total accuracy: 0.9082125603864735\n",
      "label: 0  accuracy: 0.7727272727272727\n",
      "label: 1  accuracy: 0.96\n",
      "label: 2  accuracy: 0.9166666666666666\n",
      "label: 3  accuracy: 0.7894736842105263\n",
      "label: 4  accuracy: 0.9545454545454546\n",
      "label: 5  accuracy: 0.8333333333333334\n",
      "label: 6  accuracy: 1.0\n",
      "label: 7  accuracy: 1.0\n",
      "label: 8  accuracy: 0.9\n",
      "label: 9  accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 00:45:01.151734: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-07-10 00:45:01.151755: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-07-10 00:45:01.151906: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/yn/ntp3qk1946g09h3hckj58phm0000gp/T/tmp8ekk6u17\n",
      "2022-07-10 00:45:01.153995: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-07-10 00:45:01.154019: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/yn/ntp3qk1946g09h3hckj58phm0000gp/T/tmp8ekk6u17\n",
      "2022-07-10 00:45:01.160399: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-07-10 00:45:01.216055: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/yn/ntp3qk1946g09h3hckj58phm0000gp/T/tmp8ekk6u17\n",
      "2022-07-10 00:45:01.234603: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 82699 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# 训练全连接网络模型\n",
    "model = train_net(train_x, train_y, batch_size=16, epochs=24, load_save_model=False)\n",
    "verify_model(model, vX, vY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07c42876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train create new model\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 32)        320       \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 16)          4624      \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 576)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                18464     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,738\n",
      "Trainable params: 23,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/16\n",
      "2098/2098 [==============================] - 11s 5ms/step - loss: 1.4439 - accuracy: 0.4979\n",
      "Epoch 2/16\n",
      "2098/2098 [==============================] - 11s 5ms/step - loss: 0.8131 - accuracy: 0.7372\n",
      "Epoch 3/16\n",
      "2098/2098 [==============================] - 15s 7ms/step - loss: 0.6257 - accuracy: 0.7974\n",
      "Epoch 4/16\n",
      "2098/2098 [==============================] - 11s 5ms/step - loss: 0.5107 - accuracy: 0.8364\n",
      "Epoch 5/16\n",
      "2098/2098 [==============================] - 12s 6ms/step - loss: 0.4300 - accuracy: 0.8635\n",
      "Epoch 6/16\n",
      "2098/2098 [==============================] - 13s 6ms/step - loss: 0.3712 - accuracy: 0.8806\n",
      "Epoch 7/16\n",
      "2098/2098 [==============================] - 12s 6ms/step - loss: 0.3252 - accuracy: 0.8955\n",
      "Epoch 8/16\n",
      "2098/2098 [==============================] - 11s 5ms/step - loss: 0.2889 - accuracy: 0.9054\n",
      "Epoch 9/16\n",
      "2098/2098 [==============================] - 11s 5ms/step - loss: 0.2568 - accuracy: 0.9158\n",
      "Epoch 10/16\n",
      "2098/2098 [==============================] - 12s 6ms/step - loss: 0.2319 - accuracy: 0.9232\n",
      "Epoch 11/16\n",
      "2098/2098 [==============================] - 12s 6ms/step - loss: 0.2113 - accuracy: 0.9291\n",
      "Epoch 12/16\n",
      "2098/2098 [==============================] - 11s 5ms/step - loss: 0.1971 - accuracy: 0.9329\n",
      "Epoch 13/16\n",
      "2098/2098 [==============================] - 12s 6ms/step - loss: 0.1749 - accuracy: 0.9407\n",
      "Epoch 14/16\n",
      "2098/2098 [==============================] - 13s 6ms/step - loss: 0.1631 - accuracy: 0.9452\n",
      "Epoch 15/16\n",
      "2098/2098 [==============================] - 12s 6ms/step - loss: 0.1498 - accuracy: 0.9488\n",
      "Epoch 16/16\n",
      "2098/2098 [==============================] - 11s 5ms/step - loss: 0.1406 - accuracy: 0.9522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model-cnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model-cnn/assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/yn/ntp3qk1946g09h3hckj58phm0000gp/T/tmpspnn9te9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/yn/ntp3qk1946g09h3hckj58phm0000gp/T/tmpspnn9te9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step\n",
      "total accuracy: 0.966183574879227\n",
      "label: 0  accuracy: 0.9545454545454546\n",
      "label: 1  accuracy: 0.92\n",
      "label: 2  accuracy: 1.0\n",
      "label: 3  accuracy: 1.0\n",
      "label: 4  accuracy: 1.0\n",
      "label: 5  accuracy: 0.9166666666666666\n",
      "label: 6  accuracy: 1.0\n",
      "label: 7  accuracy: 1.0\n",
      "label: 8  accuracy: 1.0\n",
      "label: 9  accuracy: 0.8333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 00:48:15.445869: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-07-10 00:48:15.445917: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-07-10 00:48:15.446077: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/yn/ntp3qk1946g09h3hckj58phm0000gp/T/tmpspnn9te9\n",
      "2022-07-10 00:48:15.448569: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-07-10 00:48:15.448592: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/yn/ntp3qk1946g09h3hckj58phm0000gp/T/tmpspnn9te9\n",
      "2022-07-10 00:48:15.459114: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-07-10 00:48:15.545525: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/yn/ntp3qk1946g09h3hckj58phm0000gp/T/tmpspnn9te9\n",
      "2022-07-10 00:48:15.572289: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 126212 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# 训练CNN卷积神经网络模型\n",
    "model = train_net(train_x, train_y, batch_size=16, epochs=16, use_cnn=True, load_save_model=False)\n",
    "verify_model(model, vX, vY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9353d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain model\n",
    "#train_net(train_x, train_y, batch_size=16, epochs=8, load_save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5dded4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 数据增广测试代码，第一张为原图，其他为变换后图片\n",
    "import torch\n",
    "import torchvision\n",
    "org = Image.open(\"./dataset/0/2022_06_28_08_27_39_796.png\")\n",
    "w, h, c = (*org.size, len(org.mode))\n",
    "trans = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomAffine(translate=(0.25, 0.25), scale=(0.8, 1.2), degrees=40, interpolation=torchvision.transforms.functional.InterpolationMode.NEAREST),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.2),\n",
    "    torchvision.transforms.RandomCrop(size=(h, w))])\n",
    "\n",
    "plt.figure()\n",
    "for i in range(10):\n",
    "    if i == 0:\n",
    "        img = org\n",
    "    else:\n",
    "        img = trans(org)\n",
    "    img = img.resize((28, 28), Image.ANTIALIAS)\n",
    "    gray = img.convert(\"L\")\n",
    "    plt.subplot(4, 8, i+1)\n",
    "    plt.imshow(gray)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
